{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wattwatchers/le_completeness_analysis/blob/main/le_completeness_analysis/analysis_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device id(s) configuration.\n",
    "\n",
    "There are 2 options to configure devices for analysis.\n",
    "1. All devices associated with the API key configured in the .env file\n",
    "2. A list of device ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key for the Public REST API\n",
    "# If you want to analyse all devices associated with the API key, set the value of DEVICE_IDS below to an empty list ([])\n",
    "API_KEY: str = \"\"\n",
    "\n",
    "# If you want to analyse a subset of devices, enter the device ids inside the brackets ([]) like this:\n",
    "# [ \"DD1234567890\", \"DD2345678901\"]\n",
    "\n",
    "DEVICE_IDS: list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Period configuration\n",
    "\n",
    "Configure the time period to analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEZONE: str =   \"Australia/Sydney\"  # The timezone the period is defined in\n",
    "START_DATE: str = \"2025-02-20\"        # Period starts at start of START_DATE (date string in the format <YYYY-MM-DD>)\n",
    "END_DATE: str =   \"2025-03-27\"        # Period ends at the end of END_DATE (ate string in the format <YYYY-MM-DD>)\n",
    "                                      # Has to be today or earlier. \n",
    "                                      # If today, analysis will be up to the most recent 5 minute boundary at the time the notebook is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold configuration\n",
    "\n",
    "Configure the data completeness threshold (as a percentage) under which a device is considered problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COMPLETENESS_THRESHOLD: float = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other config (you probably won't need to touch these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TPS: int                    = 10            # Maximum # of API requests per second\n",
    "ENVIRONMENT: str                = \"production\"  # API environment (staging or production)\n",
    "ANALYSE_UNAGGREGATED_DATA: bool = True          # When False, only data aggregated to daily granularity is generated and analysed\n",
    "                                                # When True, data is also kept and analysed at 5m granularity and. Only do this for small sets of devices / short time periods\n",
    "                                                # When 50 or more devices are analysed, unaggregated data will not be analysed, regardless of this configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pandas as pd\n",
    "%pip install pendulum\n",
    "import pendulum\n",
    "%pip install itables\n",
    "from itables import show, JavascriptFunction, JavascriptCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(logging_level: str = \"INFO\") -> logging.Logger:\n",
    "    logger = logging.getLogger(\"notebook\")\n",
    "    logger.setLevel(logging_level)\n",
    "\n",
    "    # loggers are cached, so if we call this from multiple places we end up with multiple handlers\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    stdout_handler = logging.StreamHandler()\n",
    "    formatter: logging.Formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n",
    "    stdout_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stdout_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partialmethod\n",
    "from typing import Any\n",
    "\n",
    "import httpx\n",
    "from pendulum import DateTime\n",
    "\n",
    "JSONType = None | bool | int | float | str | list[Any] | dict[str, Any]\n",
    "\n",
    "\n",
    "class RestError(Exception):\n",
    "    \"\"\"\n",
    "    Error from REST API Client.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        request: httpx.Request,\n",
    "        response: httpx.Response | None = None,\n",
    "    ):\n",
    "        super().__init__(message)\n",
    "        self.request: httpx.Request = request\n",
    "        self.response: httpx.Response | None = response\n",
    "\n",
    "\n",
    "class RestAPIClient:\n",
    "\n",
    "    def __init__(self, base_url: str, requests_per_sec_max: int, **session_kwargs):\n",
    "        self._base_url = base_url\n",
    "        self._client = httpx.Client()\n",
    "        self._requests_per_sec_max = requests_per_sec_max\n",
    "        self._last_request_time: DateTime | None = None\n",
    "\n",
    "        for key, value in session_kwargs.items():\n",
    "            setattr(self._client, key, value)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        self.close()\n",
    "\n",
    "    def close(self):\n",
    "        return self._client.close()\n",
    "\n",
    "    def _throttler(self):\n",
    "        \"\"\"\n",
    "        This method throttles API request based on when the last request was made and the number of maximum number of requests per second configured.\n",
    "        (the actual frequency of requests can be lower than the maximum allowed if requests take longer to complete than the minimum interval\n",
    "        between requests)\n",
    "        \"\"\"\n",
    "        if self._last_request_time is None:\n",
    "            return\n",
    "        time_since_last_request = self._last_request_time.diff().total_seconds()\n",
    "        wait_duration = max(0, 1 / self._requests_per_sec_max - time_since_last_request)\n",
    "        if wait_duration > 0:\n",
    "            time.sleep(wait_duration)\n",
    "\n",
    "    def request(self, method: str, path: str, **kwargs) -> tuple[JSONType, RestError]:\n",
    "        self._throttler()\n",
    "        try:\n",
    "            resp = self._client.request(method, f\"{self._base_url}/{path}\", **kwargs)\n",
    "            resp.raise_for_status()\n",
    "            if len(resp.text) == 0:\n",
    "                return None, None\n",
    "            return (resp.json(), None)\n",
    "        except httpx.HTTPStatusError as http_error:\n",
    "            error_message = http_error.response.json().get(\"message\", \"\")\n",
    "            error = RestError(\n",
    "                f\"Error response {http_error.response.status_code} while requesting {http_error.request.url!r}: {error_message}\",\n",
    "                http_error.request,\n",
    "                http_error.response,\n",
    "            )\n",
    "            return (None, error)\n",
    "        except httpx.RequestError as err:\n",
    "            error = RestError(\n",
    "                f\"An error occurred while requesting {err.request.url!r}.\", err.request\n",
    "            )\n",
    "            return (None, error)\n",
    "        finally:\n",
    "            self._last_request_time = pendulum.now()\n",
    "\n",
    "    get = partialmethod(request, \"GET\")\n",
    "    post = partialmethod(request, \"POST\")\n",
    "    put = partialmethod(request, \"PUT\")\n",
    "    patch = partialmethod(request, \"PATCH\")\n",
    "    delete = partialmethod(request, \"DELETE\")\n",
    "    head = partialmethod(request, \"HEAD\")\n",
    "    options = partialmethod(request, \"OPTIONS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class TimeInterval:\n",
    "    \"\"\"\n",
    "    Data class for a time interval\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp_start: int\n",
    "    timestamp_end: int\n",
    "\n",
    "\n",
    "class Granularity(Enum):\n",
    "    \"\"\"\n",
    "    Enum for different LE granularities\n",
    "    \"\"\"\n",
    "\n",
    "    FIVE_MINS = \"5m\"\n",
    "    FIFTEEN_MINS = \"15m\"\n",
    "    THIRTY_MINS = \"30m\"\n",
    "    HOUR = \"hour\"\n",
    "    DAY = \"day\"\n",
    "    WEEK = \"week\"\n",
    "    MONTH = \"month\"\n",
    "\n",
    "\n",
    "class CallerError(Exception):\n",
    "    \"\"\"\n",
    "    Error by caller of the client.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class PublicApiClient(RestAPIClient):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        environment: str,\n",
    "        api_key: str,\n",
    "        requests_per_sec_max: int,\n",
    "        logger: logging.Logger,\n",
    "    ):\n",
    "        match environment:\n",
    "            case \"production\" | \"prod\":\n",
    "                base_url = \"https://api-v3.wattwatchers.com.au\"\n",
    "            case \"staging\":\n",
    "                base_url = \"https://api-v3-stage.wattwatchers.com.au\"\n",
    "            case _:\n",
    "                # fallback is prod\n",
    "                base_url = \"https://api-v3.wattwatchers.com.au\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        # TODO: use a considered value for number of requests per second\n",
    "        super().__init__(base_url, requests_per_sec_max, headers=headers)\n",
    "        self._logger = logger\n",
    "\n",
    "    def get_devices_list(self) -> tuple[list | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Retrieves all device ids associated with the API key\n",
    "        \"\"\"\n",
    "        result = super().get(\"devices\")\n",
    "        return result\n",
    "\n",
    "    def get_device_status(self, device_id: str) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Retrieves the status of the device associated with the device_id\n",
    "        \"\"\"\n",
    "        result = super().get(f\"devices/{device_id}\")\n",
    "        return result\n",
    "\n",
    "    def patch_device_status(\n",
    "        self, device_id: str, payload: dict\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Patches the device status of the device associated with the device_id\n",
    "        Used (among other things) to update WiFi credentials\n",
    "        \"\"\"\n",
    "        result = super().patch(f\"devices/{device_id}\", data=json.dumps(payload))\n",
    "        return result\n",
    "\n",
    "    def update_wifi_credentials(\n",
    "        self, device_id: str, ssid: str | None = None, psk: str | None = None\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Updates the WiFi credentials of the device associated with the device_id\n",
    "        If successful, this will cause the device to switch to WiFi comms.\n",
    "        \"\"\"\n",
    "        if ssid is None and psk is None:\n",
    "            # No credential details provided, return error\n",
    "            return (\n",
    "                None,\n",
    "                CallerError(\n",
    "                    \"Request to update WiFi credentials requires at least one of SSID and PSK to be defined.\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        payload = {\"comms\": {\"wifi\": {}}}\n",
    "        if not (ssid is None):\n",
    "            payload[\"comms\"][\"wifi\"][\"ssid\"] = ssid\n",
    "        if not (psk is None):\n",
    "            payload[\"comms\"][\"wifi\"][\"psk\"] = psk\n",
    "\n",
    "        return self.patch_device_status(device_id, payload)\n",
    "\n",
    "    def reset_wifi_credentials(\n",
    "        self, device_id: str\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Resets the WiFi credentials of the device associated with the device_id\n",
    "        This will cause the device to switch to cellular comms.\n",
    "        \"\"\"\n",
    "        return self.update_wifi_credentials(device_id, \"\", \"\")\n",
    "\n",
    "    def change_switch_state(\n",
    "        self, device_id: str, switch_id: str, target_state: str\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Changes the switch state of the switch with id `switch_id` on the device with\n",
    "        id `device_id` to state `target_state`.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"id\": device_id,\n",
    "            \"switches\": [{\"id\": switch_id, \"state\": target_state}],\n",
    "        }\n",
    "        return self.patch_device_status(device_id, payload)\n",
    "\n",
    "    def update_se_reporting_interval(\n",
    "        self, device_id: str, reporting_interval: int\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Update the SE reporting interval for the device to the requested value\n",
    "        \"\"\"\n",
    "        payload = {\"shortEnergyReportingInterval\": reporting_interval}\n",
    "        return super().post(\n",
    "            f\"devices/{device_id}/reporting-interval\", data=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "    def get_latest_se(\n",
    "        self, device_id: str, energy_unit: str | None = \"kW\"\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        if energy_unit is not None and energy_unit in [\"kW\", \"kWh\"]:\n",
    "            params = {\"convert[energy]\": energy_unit}\n",
    "            return super().get(f\"short-energy/{device_id}/latest\", params=params)\n",
    "        return super().get(f\"short-energy/{device_id}/latest\")\n",
    "\n",
    "    def _max_interval_for_granularity(self, granularity: Granularity) -> int:\n",
    "        \"\"\"\n",
    "        Returns the maximum interval for a single energy request based on the granularity\n",
    "        \"\"\"\n",
    "        MAX_INTERVALS_DAYS = {\n",
    "            Granularity.FIVE_MINS: 7,\n",
    "            Granularity.FIFTEEN_MINS: 14,\n",
    "            Granularity.THIRTY_MINS: 31,\n",
    "            Granularity.HOUR: 90,\n",
    "            Granularity.DAY: 3 * 365,  # ≈ 3 years\n",
    "            Granularity.WEEK: 5 * 365,  # ≈ 5 years\n",
    "            Granularity.MONTH: 10 * 365,  # ≈ 10 yers\n",
    "        }\n",
    "        return MAX_INTERVALS_DAYS.get(granularity, 7) * 24 * 3600\n",
    "\n",
    "    def _calculate_intervals_for(\n",
    "        self, granularity: Granularity, timestamp_start: int, timestamp_end: int\n",
    "    ) -> list[tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Batches an interval based on the maximum interval per request for the given granularity.\n",
    "        \"\"\"\n",
    "        batch_interval = self._max_interval_for_granularity(granularity)\n",
    "        intervals = [\n",
    "            TimeInterval(batch_start, min(batch_start + batch_interval, timestamp_end))\n",
    "            for batch_start in range(timestamp_start, timestamp_end, batch_interval)\n",
    "        ]\n",
    "        return intervals\n",
    "\n",
    "    def _load_energy(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        device_id: str,\n",
    "        intervals: list[TimeInterval],\n",
    "        unit: str = \"kWh\",\n",
    "        granularity: Granularity | None = None,\n",
    "    ) -> tuple[list | None, RestError | None]:\n",
    "\n",
    "        energy_data = []\n",
    "        for interval in intervals:\n",
    "            params = {\n",
    "                \"fromTs\": interval.timestamp_start,\n",
    "                \"toTs\": interval.timestamp_end,\n",
    "                \"convert[energy]\": unit,\n",
    "            }\n",
    "            if granularity is not None:\n",
    "                params[\"granularity\"] = granularity.value\n",
    "\n",
    "            self._logger.info(\n",
    "                f\"load from {interval.timestamp_start} to {interval.timestamp_end} for {device_id}\"\n",
    "            )\n",
    "            (result, error) = super().get(endpoint, params=params)\n",
    "            if error is not None:\n",
    "                self._logger.error(\n",
    "                    f\"Error retrieving LE data for {device_id} between {interval.timestamp_start} and {interval.timestamp_end}: {error}\"\n",
    "                )\n",
    "                return (None, error)\n",
    "\n",
    "            if result is not None:\n",
    "                energy_data.extend(result)\n",
    "\n",
    "        return (energy_data, None)\n",
    "\n",
    "    def load_long_energy(\n",
    "        self,\n",
    "        device_id: str,\n",
    "        timestamp_start: int,\n",
    "        timestamp_end: int,\n",
    "        granularity: Granularity = Granularity.FIVE_MINS,\n",
    "        unit: str = \"kWh\",\n",
    "    ) -> tuple[list | None, RestError | None]:\n",
    "\n",
    "        intervals = self._calculate_intervals_for(\n",
    "            granularity, timestamp_start, timestamp_end\n",
    "        )\n",
    "        return self._load_energy(\n",
    "            f\"long-energy/{device_id}\", device_id, intervals, unit, granularity\n",
    "        )\n",
    "\n",
    "    def get_first_le(self, device_id: str) -> tuple[list | None, RestError | None]:\n",
    "        result = super().get(f\"long-energy/{device_id}/first\")\n",
    "        return result\n",
    "\n",
    "    def load_short_energy(\n",
    "        self,\n",
    "        device_id: str,\n",
    "        timestamp_start: int,\n",
    "        timestamp_end: int,\n",
    "        unit: str = \"kWh\",\n",
    "    ) -> tuple[list | None, RestError | None]:\n",
    "\n",
    "        max_interval = 12 * 3600  # maximum request interval for SE is 12 hours\n",
    "        intervals = [\n",
    "            TimeInterval(batch_start, min(batch_start + max_interval, timestamp_end))\n",
    "            for batch_start in range(timestamp_start, timestamp_end, max_interval)\n",
    "        ]\n",
    "        return self._load_energy(\n",
    "            f\"short-energy/{device_id}\", device_id, intervals, unit\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger: logging.Logger = get_logger()\n",
    "public_api_client = PublicApiClient(ENVIRONMENT, API_KEY, MAX_TPS, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_device_id(device_id: str, raise_error: bool = False) -> tuple[bool, str]:\n",
    "  \"\"\"\n",
    "  Returns True if the device ID is valid, False if not.\n",
    "  Simplified version based on lib_common\n",
    "  \"\"\"\n",
    "  DEVICE_ID_PATTERN = \"^[B-F]{1}[A-F0-9]{12}$\"\n",
    "  DEVICE_ID_REGEX = re.compile(DEVICE_ID_PATTERN)\n",
    "  # re.match won't detect trailing space in the device id, but re.fullmatch will.\n",
    "  if not DEVICE_ID_REGEX.fullmatch(device_id):\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "# Determine devices to analyse\n",
    "devices: list[str] = []\n",
    "if DEVICE_IDS is not None and len(DEVICE_IDS) > 0:\n",
    "  devices = DEVICE_IDS\n",
    "else:\n",
    "  # get all devices associated with API key\n",
    "  result, error = public_api_client.get_devices_list()\n",
    "  if error is not None:\n",
    "    logger.error(f'failed to load devices for API key: {error}')\n",
    "  else:\n",
    "    devices = result\n",
    "\n",
    "# filter out any invalid device ids\n",
    "devices =[d for d in devices if is_valid_device_id(d)]\n",
    "\n",
    "num_devices = len(devices)\n",
    "logger.info(f'found {num_devices} devices to analyse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = pendulum.parse(START_DATE, tz=TIMEZONE)\n",
    "time_end = pendulum.parse(END_DATE, tz=TIMEZONE).add(days=1)\n",
    "\n",
    "def get_latest_5_min_boundary(dt):\n",
    "    # Subtract the remainder of minutes to align to the nearest 5-minute boundary\n",
    "    minutes_to_subtract = dt.minute % 5\n",
    "    adjusted_time = dt.subtract(minutes=minutes_to_subtract, seconds=dt.second, microseconds=dt.microsecond)\n",
    "    return adjusted_time\n",
    "\n",
    "if time_start > time_end:\n",
    "    raise ValueError(f\"Start date ({START_DATE}) has to be earlier than end date ({END_DATE})\")\n",
    "if time_end > pendulum.now(tz=TIMEZONE):\n",
    "    if time_end > pendulum.now(tz=TIMEZONE).end_of(\"day\").add(seconds=1):\n",
    "        raise ValueError(f\"End date ({END_DATE}) needs to be in the past\")\n",
    "    else:\n",
    "        # Adjust time_end to now (or previous 5m boundary to make life easier on ourselves)\n",
    "        time_end = get_latest_5_min_boundary(pendulum.now(tz=TIMEZONE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load device status for each device to exclude decommissioned devices\n",
    "# Issue: only user-apps-api exposes this and we can't use that because of different auth system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform interval data\n",
    "def process_interval_data(device_id:str, interval_data:list) -> pd.DataFrame:\n",
    "\n",
    "  def flatten_arrays(item: dict) -> dict:\n",
    "      \"\"\" flatten each element of arrays to their own key. Other types of values are left untouched.\n",
    "      e.g. {key: [value0, value1, ...]} becomes {key_0: value0, key_1: value1, ...}.            \n",
    "      \"\"\"\n",
    "      flattened = {}\n",
    "      for key, value in item.items():\n",
    "        if isinstance(value, list):\n",
    "          for idx, subvalue in enumerate(value):\n",
    "            flattened[f\"{key}_{idx}\"] = subvalue\n",
    "        else:\n",
    "          flattened[key] = value\n",
    "      return flattened\n",
    "\n",
    "  intervals = []\n",
    "  for item in interval_data:\n",
    "      row = flatten_arrays(item)\n",
    "      intervals.append(row)\n",
    "\n",
    "  df_intervals = pd.DataFrame.from_dict(intervals) \n",
    "  \n",
    "  df_intervals[\"device_id\"] = device_id\n",
    "  # Reorder the columns to move 'device_id', 'timestamp', and 'duration' to the front\n",
    "  columns_order = ['device_id', 'timestamp', 'duration'] + [col for col in df_intervals.columns if col not in ['device_id', 'timestamp', 'duration']]\n",
    "  df_intervals = df_intervals[columns_order]\n",
    "\n",
    "  return df_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By-day analysis\n",
    "def roll_up_to_daily(df_intervals: pd.DataFrame) -> pd.DataFrame:\n",
    "  df_intervals['datetime_end'] = pd.to_datetime(df_intervals['timestamp'], unit='s').dt.tz_localize('UTC').dt.tz_convert(TIMEZONE)\n",
    "  df_intervals['datetime_start'] = pd.to_datetime(df_intervals['timestamp'] - 300, unit='s').dt.tz_localize('UTC').dt.tz_convert(TIMEZONE)\n",
    "\n",
    "  # Aggregate based on `datetime_start` to ensure intervals are attributed to the correct day\n",
    "  df_daily_counts = df_intervals.groupby(['device_id', df_intervals['datetime_start'].dt.date]).size().reset_index(name='entry_count')\n",
    "  df_daily_counts.columns = ['device_id', 'date', 'num_intervals']\n",
    "  df_daily_counts['date'] = pd.to_datetime(df_daily_counts['date']).dt.tz_localize(TIMEZONE)\n",
    "\n",
    "  return df_daily_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_days(df: pd.DataFrame | None, device_id: str, time_start: DateTime, time_end: DateTime) -> pd.DataFrame:\n",
    "  interval = pendulum.interval(time_start, time_end.subtract(seconds=1))\n",
    "  date_range = interval.range('days')\n",
    "  missing_entries = set([d for d in date_range])\n",
    "  if df is not None:\n",
    "    missing_entries = set([d for d in date_range]).difference(set(df['date'].unique()))\n",
    "  missing_df = pd.DataFrame(list(missing_entries), columns=['date'])\n",
    "  missing_df['device_id'] = device_id\n",
    "  missing_df['num_intervals'] = 0\n",
    "  df = df if missing_df.empty else pd.concat([df, missing_df], ignore_index=True)\n",
    "  return df\n",
    "\n",
    "\n",
    "def add_completeness(df: pd.DataFrame, timestamp_first_le: int, timestamp_end: int) -> pd.DataFrame:\n",
    "  num_intervals_expected_daily = 24 * 12\n",
    "  df['num_intervals_expected'] = num_intervals_expected_daily\n",
    "\n",
    "  df['date'] = pd.to_datetime(df['date'], utc=True).dt.tz_convert(TIMEZONE)\n",
    "  # temporarily add timestamp columns based on date\n",
    "  df['timestamp_start'] = df['date'].apply(lambda x: int(x.timestamp()))\n",
    "  df['timestamp_end'] = df['timestamp_start'] + 24 * 3600\n",
    "  # Adjust timestamp_start for end-of-interval timestamps\n",
    "  df['timestamp_start'] = df['timestamp_start'] - 300\n",
    "\n",
    "  # HANDLE PARTIAL DAYS AT START OF INTERVAL\n",
    "\n",
    "  # when timestamp_end < day of timestamp_first_le, num_intervals_expected = 0\n",
    "  df.loc[df['timestamp_end'] < timestamp_first_le, 'num_intervals_expected'] = 0\n",
    "\n",
    "  # when timestamp_first_le is between timestamp_start and timestamp_end, \n",
    "  # #num_intervals_expected depends on timestamp_first_le (i.e. num intervals betweem timestamp_first_le and end of day) \n",
    "  mask = (df['timestamp_start'] <= timestamp_first_le) & (df['timestamp_end'] > timestamp_first_le)\n",
    "  # The +1 is there becuause if timestamp_first_le is equal to timestamp_end, that means there is 1 expected interval (the one represented by timestamp_first_le)\n",
    "  # The same logic holds if timesmtap_fitst_le is smaller than timestamp_end\n",
    "  df.loc[mask, 'num_intervals_expected'] =  ((df.loc[mask, 'timestamp_end'] - timestamp_first_le) // 300).astype(int) + 1\n",
    "\n",
    "  # HANDLE PARTIAL DAY AT END OF INTERVAL\n",
    "  mask = (df['timestamp_end'] > timestamp_end)\n",
    "  # For partial day at end of interval, follow same logic as for partial days at start of interval \n",
    "  # (but subtract the number of intervals between end of day and timestamp_end from the expected number of intervals for a full day)\n",
    "  df.loc[mask, 'num_intervals_expected'] = df.loc[mask, 'num_intervals_expected'] - (((df.loc[mask, 'timestamp_end'] - timestamp_end) // 300).astype(int) + 1)\n",
    "  \n",
    "\n",
    "  df['missing_intervals'] = df['num_intervals_expected'] - df['num_intervals']\n",
    "  df['interval_completeness'] = df.apply(\n",
    "      lambda row: 100 if row['num_intervals_expected'] == 0 else 100 * row['num_intervals'] / row['num_intervals_expected'], \n",
    "      axis=1\n",
    "  )\n",
    "  df['date'] = df['date'].dt.date\n",
    "  return df\n",
    "\n",
    "def move_column_to_index(df: pd.DataFrame, column_name: str, index: int) -> pd.DataFrame:\n",
    "  cols = df.columns.tolist()\n",
    "  # TODO: add check for column name and num columns\n",
    "  cols.insert(index, cols.pop(cols.index(column_name)))\n",
    "  df = df[cols]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LE data\n",
    "def get_le_request_interval(first_le_dict: dict | None, timestamp_start: int, timestamp_end: int) -> tuple | None:\n",
    "  # If result *is* None, there is no first LE i.e. device not initialised\n",
    "  # In that case we don't need to query LE and expected num intervals for whole period is 0\n",
    "  if first_le_dict is None:\n",
    "    return None\n",
    "  else:\n",
    "    first_le_timestamp = first_le_dict['timestamp']\n",
    "    if first_le_timestamp > timestamp_end:\n",
    "      # Device not initialised in the requested period\n",
    "      # No need to query LE and and expected num intervals for whole period is 0\n",
    "      return None\n",
    "    elif first_le_timestamp > timestamp_start:\n",
    "      # Partial data for the requested interval\n",
    "      # Request from first_le_timestamp, expected num intervals up to first_le_timestamp is 0\n",
    "      return first_le_timestamp, timestamp_end\n",
    "    else:\n",
    "      # Request whole requested interval, expected num intervals is all intervals in the requested interval.\n",
    "      return timestamp_start, timestamp_end\n",
    "\n",
    "\n",
    "# Add 5 minutes to time_start and time_end to adjust for fact that interval are *up to* timestamp\n",
    "timestamp_start = time_start.add(minutes=5).int_timestamp\n",
    "timestamp_end = time_end.add(minutes=5).int_timestamp\n",
    "\n",
    "df_unaggregated: pd.DataFrame = pd.DataFrame()\n",
    "df_daily_counts: pd.DataFrame = pd.DataFrame()\n",
    "for index, device_id in enumerate(devices):\n",
    "  logger.info(f'Downloading LE data for device {index+1}/{num_devices} - {device_id}')\n",
    "  # Get first LE\n",
    "  result, error = public_api_client.get_first_le(device_id)\n",
    "  if error is not None:\n",
    "    logger.error(f'Failed to load first LE for device: {device_id}: {error}')\n",
    "  else:\n",
    "    le_request_interval = get_le_request_interval(result, timestamp_start, timestamp_end)\n",
    "    if le_request_interval is None:\n",
    "      request_timestamp_start = None\n",
    "    else:\n",
    "      request_timestamp_start, request_timestamp_end = le_request_interval\n",
    "      result, error = public_api_client.load_long_energy(device_id, request_timestamp_start, request_timestamp_end)\n",
    "      if error is not None:\n",
    "        logger.error(f'Failed to load LE for device {device_id} between {request_timestamp_start} and {request_timestamp_end}: {error}')\n",
    "      else:\n",
    "        if len(result) == 0: \n",
    "          logger.info(f\"No LE data found for {device_id}\")\n",
    "      \n",
    "    if result is None or len(result) == 0:   \n",
    "      df_device_daily = None\n",
    "    else:\n",
    "      df_device_intervals = process_interval_data(device_id, result)\n",
    "\n",
    "      # Process into days, include # intervals, # excpected, # missing\n",
    "      df_device_daily = roll_up_to_daily(df_device_intervals)\n",
    "\n",
    "      if ANALYSE_UNAGGREGATED_DATA and len(devices) < 50:\n",
    "        # Only keep columns of interest\n",
    "\n",
    "        df_device_intervals = df_device_intervals[[\"device_id\", \"timestamp\", \"duration\"]]\n",
    "        df_unaggregated = df_device_intervals if df_unaggregated.empty else pd.concat([df_unaggregated, df_device_intervals])\n",
    "      \n",
    "    df_device_daily = add_missing_days(df_device_daily, device_id, time_start, time_end)\n",
    "    df_device_daily = add_completeness(df_device_daily, request_timestamp_start, timestamp_end)\n",
    "\n",
    "    df_daily_counts = df_device_daily if df_daily_counts.empty else pd.concat([df_daily_counts, df_device_daily])\n",
    "    \n",
    "logger.info(f'Successfully downloaded LE data for {len(df_daily_counts[\"device_id\"].unique())}/{num_devices} devices')\n",
    "\n",
    "df_daily_counts = df_daily_counts.reset_index(drop=True)\n",
    "# TODO: do this on a per device basis, so we can take firstLE into account\n",
    "\n",
    "\n",
    "# Re-order columns\n",
    "df_daily_counts = move_column_to_index(df_daily_counts, 'interval_completeness', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether df_daily is empty\n",
    "if df_daily_counts.empty:\n",
    "    raise ValueError(\"No LE data was downloaded for any device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_aggregated = df_daily_counts.groupby('device_id').agg({\n",
    "    'num_intervals': 'sum',\n",
    "    'num_intervals_expected': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "df_daily_counts['date'] = pd.to_datetime(df_daily_counts['date'])\n",
    "df_daily_counts = df_daily_counts.set_index('date')\n",
    "\n",
    "# Aggregate by device by month\n",
    "df_monthly_counts = df_daily_counts.groupby(['device_id', pd.Grouper(freq='ME')]).agg({\n",
    "    'num_intervals': 'sum',\n",
    "    'num_intervals_expected': 'sum'\n",
    "}).reset_index()\n",
    "df_monthly_counts['date'] = df_monthly_counts['date'].dt.to_period('M').astype(str)\n",
    "\n",
    "# Aggregate across all devices by day\n",
    "df_daily_counts_all = df_daily_counts.groupby('date').agg({\n",
    "    'num_intervals': 'sum',\n",
    "    'num_intervals_expected': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate across all devices by month\n",
    "df_daily_counts_all['date'] = pd.to_datetime(df_daily_counts_all['date'])\n",
    "df_daily_counts_all = df_daily_counts_all.set_index('date')\n",
    "df_monthly_counts_all = df_daily_counts_all.groupby(pd.Grouper(freq='ME')).agg({\n",
    "    'num_intervals': 'sum',\n",
    "    'num_intervals_expected': 'sum'\n",
    "}).reset_index()\n",
    "df_monthly_counts_all['date'] = df_monthly_counts_all['date'].dt.to_period('M').astype(str)\n",
    "\n",
    "\n",
    "def add_completeness_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['num_intervals_missing'] = df['num_intervals_expected'] - df['num_intervals']\n",
    "    df['interval_completeness'] = 100 * df['num_intervals'] / df['num_intervals_expected']\n",
    "    # df = df.drop(columns=['num_intervals_expected'])\n",
    "    if 'date' in df.columns:\n",
    "        df = df.sort_values(by='date')\n",
    "    return df\n",
    "\n",
    "df_aggregated = add_completeness_columns(df_aggregated)\n",
    "\n",
    "df_monthly_counts = add_completeness_columns(df_monthly_counts)\n",
    "df_monthly_counts = move_column_to_index(df_monthly_counts, 'interval_completeness', 2)\n",
    "\n",
    "df_daily_counts_all = add_completeness_columns(df_daily_counts_all)\n",
    "df_daily_counts_all = df_daily_counts_all.reset_index()\n",
    "df_daily_counts_all = move_column_to_index(df_daily_counts_all, 'interval_completeness', 1)\n",
    "\n",
    "df_monthly_counts_all = add_completeness_columns(df_monthly_counts_all)\n",
    "df_monthly_counts_all = move_column_to_index(df_monthly_counts_all, 'interval_completeness', 1)\n",
    "  \n",
    "# We can now remove `num_intervals_expected` from `df_daily_counts` so it's not included in the itable later on\n",
    "# df_daily_counts = df_daily_counts.drop(columns=['num_intervals_expected'])\n",
    "df_daily_counts = df_daily_counts.reset_index()\n",
    "\n",
    "\n",
    "# Devices we couldn't download LE data for (included in devices list but not in LE dict)\n",
    "devices_with_le_data = list(df_aggregated[\"device_id\"].unique())\n",
    "devices_without_le_data = [device_id for device_id in devices if device_id not in devices_with_le_data]\n",
    "\n",
    "# Devices with missing LE data\n",
    "# TODO: add alternative analysis based on timestamp and duration of interval (only works for intervals between existing intervals, i.e. need to handle missing intervals at start or end of period separately)\n",
    "# Could also just do a quick analysis to verify all intervals have a duration of 300s.\n",
    "devices_with_missing_le_data = df_aggregated[df_aggregated['num_intervals_missing'] > 0]['device_id'].tolist()\n",
    "\n",
    "# Devices not meeting data completeness threshold\n",
    "\n",
    "devices_not_meeting_threshold = df_aggregated[df_aggregated['interval_completeness'] < DATA_COMPLETENESS_THRESHOLD]['device_id'].tolist()\n",
    "\n",
    "# Devices with complete LE data\n",
    "devices_with_complete_le_data = df_aggregated[df_aggregated['num_intervals_missing'] == 0]['device_id'].tolist()\n",
    "\n",
    "\n",
    "# Determine expected number of intervals over the full analysed period\n",
    "# Calculate the number of 5-minute intervals between the start and end timestamps using pendulum\n",
    "num_intervals_expected = int((time_end.diff(time_start).in_minutes()) // 5)\n",
    "\n",
    "# Devices installed within analysed period\n",
    "devices_installed_in_period = df_aggregated[(df_aggregated['num_intervals_expected'] > 0) & (df_aggregated['num_intervals_expected'] < num_intervals_expected )]['device_id'].tolist() \n",
    "\n",
    "# Devices not installed at all\n",
    "devices_not_installed = df_aggregated[df_aggregated['num_intervals_expected'] == 0]['device_id'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High level analysis\n",
    "\n",
    "parameters = [{\n",
    "  'start_time': time_start,\n",
    "  'end_time': time_end,\n",
    "  'num_expected_intervals': num_intervals_expected,\n",
    "}]\n",
    "\n",
    "# TODO: update overall completeness calculation as we now take firstLE into account - so different devices have different num_expected_intervals\n",
    "top_level_stats = [{\n",
    "  'num_devices': num_devices,\n",
    "  'overall_completeness': df_aggregated['num_intervals'].sum() / df_aggregated['num_intervals_expected'].sum() * 100,\n",
    "  'devices_under_threshold': len(devices_not_meeting_threshold),\n",
    "  'devices_with_missing_intervals': len(devices_with_missing_le_data),\n",
    "  'devices_without_data': len(devices_without_le_data),\n",
    "  'devices_with_complete_data': len(devices_with_complete_le_data),\n",
    "  'devices_installed_during_analysis_period': len(devices_installed_in_period),\n",
    "  'devices_not_installed': len(devices_not_installed)\n",
    "}]\n",
    "\n",
    "df_parameters = pd.DataFrame.from_dict(parameters)\n",
    "df_stats = pd.DataFrame.from_dict(top_level_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_graph(df: pd.DataFrame) -> plotly.graph_objects.Figure:\n",
    "  # Calculate the average interval_completeness\n",
    "  average_completeness = df['interval_completeness'].mean()\n",
    "\n",
    "  # Create the plot\n",
    "  fig = px.line(\n",
    "    df, \n",
    "    x='date',\n",
    "    y='interval_completeness', \n",
    "    title='Interval Completeness Over Time'\n",
    "  )\n",
    "  # Update the x-axis and y-axis labels\n",
    "  fig.update_layout(\n",
    "      xaxis_title='Month',\n",
    "      yaxis_title='Interval Completeness (%)'\n",
    "  )\n",
    "\n",
    "  # Add a horizontal line for the average interval_completeness\n",
    "  fig.add_shape(\n",
    "      type=\"line\",\n",
    "      x0=df['date'].min(),\n",
    "      y0=average_completeness,\n",
    "      x1=df['date'].max(),\n",
    "      y1=average_completeness,\n",
    "      line=dict(\n",
    "          color=\"Red\",\n",
    "          width=2,\n",
    "          dash=\"dashdot\",\n",
    "      ),\n",
    "      name=\"Average Interval Completeness\",\n",
    "      showlegend=True\n",
    "  )\n",
    "\n",
    "  # Add the line graph to the legend\n",
    "  fig.for_each_trace(\n",
    "      lambda trace: trace.update(name='Interval Completeness'),\n",
    "  )\n",
    "  fig.update_traces(showlegend = True)\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df_parameters)\n",
    "show(df_stats, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [1], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [1],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate per month stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df_monthly_counts_all, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [1], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [1],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],\n",
    "    showIndex=False,\n",
    "    pageLength=20,\n",
    "    buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = trend_graph(df_monthly_counts_all)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate per day stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df_daily_counts_all, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [1], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [1],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],\n",
    "    showIndex=False,\n",
    "    pageLength=20,\n",
    "    buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = trend_graph(df_daily_counts_all)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per device stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated = df_aggregated[['device_id', 'interval_completeness', 'num_intervals_missing']]\n",
    "show(df_aggregated, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [1], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [1],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],\n",
    "    showIndex=False,\n",
    "    buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per device per month stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df_monthly_counts, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [2], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [2],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],\n",
    "    showIndex=False,\n",
    "    pageLength=20,\n",
    "    buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per device per day stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df_daily_counts, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [2], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [2],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],\n",
    "    showIndex=False,\n",
    "    pageLength=20,\n",
    "    buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of unaggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_unaggregated.empty:\n",
    "    raise ValueError(\"Unaggregated interval analysis not configured or too many devices selected.\")\n",
    "\n",
    "df_unaggregated = df_unaggregated.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-of-day across all devices (assumes all devices in same timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resample_dataframe(df: pd.DataFrame, resample_interval: str) -> pd.DataFrame:\n",
    "    df[\"datetime\"] = (\n",
    "        pd.to_datetime(df[\"timestamp\"], unit=\"s\").dt.tz_localize(\"UTC\").dt.tz_convert(TIMEZONE)\n",
    "    )\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    df_resampled = df.resample(resample_interval).agg(\n",
    "        duration_sum=(\"duration\", \"sum\"),\n",
    "        min_timestamp=(\"timestamp\", \"min\"),\n",
    "        max_timestamp=(\"timestamp\", \"max\"),\n",
    "        num_intervals=(\"timestamp\", \"count\"),\n",
    "    )\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_missing_intervals(df: pd.DataFrame, title: str, x: str=\"datetime\", y: str=\"num_intervals_missing\") -> go.Figure:\n",
    "    df = df.reset_index()\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        labels={\"num_intervals_missing\": \"# missing intervals\", \"datetime\": \"Time\", \"time\": \"Time\"},\n",
    "        title=title\n",
    "    )\n",
    "    missing_intervals_max = df[\"num_intervals_missing\"].max()\n",
    "    range_max = max(missing_intervals_max * 1.1, 1.1)\n",
    "    range_min = -0.1 if missing_intervals_max < 5 else -1\n",
    "    dtick = 1 if missing_intervals_max < 10 else 5\n",
    "    fig.update_yaxes(range=[range_min, range_max], dtick=dtick)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series per device\n",
    "df_unaggregated_grouped = df_unaggregated.groupby('device_id')\n",
    "num_plots: int = len(df_unaggregated_grouped)\n",
    "fig = make_subplots(\n",
    "  rows=num_plots, \n",
    "  cols=1,\n",
    "  subplot_titles=[device_id for device_id in df_unaggregated_grouped.groups.keys()]\n",
    ")\n",
    "row = 0\n",
    "for device_id, df_device in df_unaggregated_grouped:\n",
    "  row += 1\n",
    "  print(device_id)\n",
    "  df_device_resampled = _resample_dataframe(df_device, \"5min\")\n",
    "  df_device_resampled[\"num_intervals_expected\"] = 1\n",
    "  df_device_resampled[\"num_intervals_missing\"] = (df_device_resampled[\"num_intervals_expected\"] - df_device_resampled[\"num_intervals\"]).clip(lower=0)\n",
    "  device_fig = graph_missing_intervals(df_device_resampled, device_id)\n",
    "  for trace in device_fig.data:\n",
    "    fig.add_trace(trace, row=row, col=1)\n",
    "  fig.update_xaxes(title_text = \"Time\", row=row, col=1)\n",
    "  fig.update_yaxes(title_text = \"# Intervals missing\", \n",
    "                   row=row, \n",
    "                   col=1, \n",
    "                   tickvals= [*range(0, max(2, int(df_device_resampled['num_intervals_missing'].max() + 1)))]\n",
    "                  )\n",
    "  \n",
    "fig.update_layout(height=num_plots * 270 + 100)\n",
    "fig.update_layout(title=\"Missing Intervals Time Series\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-of-day per device\n",
    "\n",
    "df_unaggregated_grouped = df_unaggregated.groupby('device_id')\n",
    "num_plots: int = len(df_unaggregated_grouped)\n",
    "fig = make_subplots(\n",
    "  rows=num_plots, \n",
    "  cols=1,\n",
    "  subplot_titles=[f\"{device_id} ({START_DATE} - {END_DATE})\" for device_id in df_unaggregated_grouped.groups.keys()],\n",
    ")\n",
    "row = 0\n",
    "for device_id, df_device in df_unaggregated_grouped:\n",
    "  row += 1\n",
    "  print(device_id)\n",
    "  df_device_resampled = _resample_dataframe(df_device, \"5min\")\n",
    "  df_device_resampled[\"num_intervals_expected\"] = 1\n",
    "  df_device_resampled[\"num_intervals_missing\"] = (df_device_resampled[\"num_intervals_expected\"] - df_device_resampled[\"num_intervals\"]).clip(lower=0)\n",
    "  # Add time-of-day column\n",
    "  df_device_resampled['time'] = df_device_resampled.index.time\n",
    "  # Group by time of day and aggregate num_intervals_missing\n",
    "  df_time_of_day_grouped = df_device_resampled.groupby('time').agg(num_intervals_missing=('num_intervals_missing', 'sum')).reset_index()\n",
    "  \n",
    "  device_fig = graph_missing_intervals(df_time_of_day_grouped, f\"{device_id} ({START_DATE} - {END_DATE})\", \"time\")\n",
    "  for trace in device_fig.data:\n",
    "    fig.add_trace(trace, row=row, col=1)\n",
    "  fig.update_xaxes(title_text = \"Time of day\", row=row, col=1)\n",
    "  fig.update_yaxes(title_text = \"# Intervals missing\", \n",
    "                   row=row, col=1, \n",
    "                   tickvals= [*range(0, max(2, int(df_time_of_day_grouped['num_intervals_missing'].max() + 1)))]\n",
    "                  )\n",
    "\n",
    "fig.update_layout(height=num_plots * 270 + 100)\n",
    "fig.update_layout(title=\"Missing Intervals by Time of Day\")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "le-completeness-analysis-XK_cgeV8-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
