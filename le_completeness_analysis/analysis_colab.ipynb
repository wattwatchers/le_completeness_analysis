{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wattwatchers/le_completeness_analysis/blob/main/le_completeness_analysis/analysis_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device id(s) configuration.\n",
    "\n",
    "There are 2 options to configure devices for analysis.\n",
    "1. All devices associated with the API key configured in the .env file\n",
    "2. A list of device ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key for the Public REST API\n",
    "# If you want to analyse all devices associated with the API key, set the value of DEVICE_IDS below to an empty list ([])\n",
    "API_KEY: str = \"\"\n",
    "\n",
    "# If you want to analyse a subset of devices, enter the device ids inside the brackets ([]) like this:\n",
    "# [ \"DD1234567890\", \"DD2345678901\"]\n",
    "DEVICE_IDS: list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Period configuration\n",
    "\n",
    "Configure the time period to analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEZONE: str = \"Australia/Sydney\"         # The timezone the period is defined in\n",
    "START_DATE: str = \"2025-02-27\"             # Date string in the format <YYYY-MM-DD> in the target timezone\n",
    "END_DATE: str = \"2025-03-25\"               # Date string in the format <YYYY-MM-DD> in the target timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold configuration\n",
    "\n",
    "Configure the data completeness threshold (as a percentage) under which a device is considered problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COMPLETENESS_THRESHOLD: float = 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other config (you probably won't need to touch these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TPS: int = 4                 # Maximum # of API requests per second\n",
    "ENVIRONMENT: str = \"production\"  # API environment (staging or production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "%pip install pendulum\n",
    "import pendulum\n",
    "%pip install itables\n",
    "from itables import show, JavascriptFunction, JavascriptCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(logging_level: str = \"INFO\") -> logging.Logger:\n",
    "    logger = logging.getLogger(\"notebook\")\n",
    "    logger.setLevel(logging_level)\n",
    "\n",
    "    # loggers are cached, so if we call this from multiple places we end up with multiple handlers\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    stdout_handler = logging.StreamHandler()\n",
    "    formatter: logging.Formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n",
    "    stdout_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stdout_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partialmethod\n",
    "from typing import Any\n",
    "\n",
    "import httpx\n",
    "from pendulum import DateTime\n",
    "\n",
    "JSONType = None | bool | int | float | str | list[Any] | dict[str, Any]\n",
    "\n",
    "\n",
    "class RestError(Exception):\n",
    "    \"\"\"\n",
    "    Error from REST API Client.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        request: httpx.Request,\n",
    "        response: httpx.Response | None = None,\n",
    "    ):\n",
    "        super().__init__(message)\n",
    "        self.request: httpx.Request = request\n",
    "        self.response: httpx.Response | None = response\n",
    "\n",
    "\n",
    "class RestAPIClient:\n",
    "\n",
    "    def __init__(self, base_url: str, requests_per_sec_max: int, **session_kwargs):\n",
    "        self._base_url = base_url\n",
    "        self._client = httpx.Client()\n",
    "        self._requests_per_sec_max = requests_per_sec_max\n",
    "        self._last_request_time: DateTime | None = None\n",
    "\n",
    "        for key, value in session_kwargs.items():\n",
    "            setattr(self._client, key, value)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        self.close()\n",
    "\n",
    "    def close(self):\n",
    "        return self._client.close()\n",
    "\n",
    "    def _throttler(self):\n",
    "        \"\"\"\n",
    "        This method throttles API request based on when the last request was made and the number of maximum number of requests per second configured.\n",
    "        (the actual frequency of requests can be lower than the maximum allowed if requests take longer to complete than the minimum interval\n",
    "        between requests)\n",
    "        \"\"\"\n",
    "        if self._last_request_time is None:\n",
    "            return\n",
    "        time_since_last_request = self._last_request_time.diff().in_seconds()\n",
    "        wait_duration = max(0, 1 / self._requests_per_sec_max - time_since_last_request)\n",
    "        if wait_duration > 0:\n",
    "            time.sleep(wait_duration)\n",
    "\n",
    "    def request(self, method: str, path: str, **kwargs) -> tuple[JSONType, RestError]:\n",
    "        self._throttler()\n",
    "        try:\n",
    "            resp = self._client.request(method, f\"{self._base_url}/{path}\", **kwargs)\n",
    "            resp.raise_for_status()\n",
    "            if len(resp.text) == 0:\n",
    "                return None, None\n",
    "            return (resp.json(), None)\n",
    "        except httpx.HTTPStatusError as http_error:\n",
    "            error_message = http_error.response.json().get(\"message\", \"\")\n",
    "            error = RestError(\n",
    "                f\"Error response {http_error.response.status_code} while requesting {http_error.request.url!r}: {error_message}\",\n",
    "                http_error.request,\n",
    "                http_error.response,\n",
    "            )\n",
    "            return (None, error)\n",
    "        except httpx.RequestError as err:\n",
    "            error = RestError(\n",
    "                f\"An error occurred while requesting {err.request.url!r}.\", err.request\n",
    "            )\n",
    "            return (None, error)\n",
    "        finally:\n",
    "            self._last_request_time = pendulum.now()\n",
    "\n",
    "    get = partialmethod(request, \"GET\")\n",
    "    post = partialmethod(request, \"POST\")\n",
    "    put = partialmethod(request, \"PUT\")\n",
    "    patch = partialmethod(request, \"PATCH\")\n",
    "    delete = partialmethod(request, \"DELETE\")\n",
    "    head = partialmethod(request, \"HEAD\")\n",
    "    options = partialmethod(request, \"OPTIONS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class TimeInterval:\n",
    "    \"\"\"\n",
    "    Data class for a time interval\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp_start: int\n",
    "    timestamp_end: int\n",
    "\n",
    "\n",
    "class Granularity(Enum):\n",
    "    \"\"\"\n",
    "    Enum for different LE granularities\n",
    "    \"\"\"\n",
    "\n",
    "    FIVE_MINS = \"5m\"\n",
    "    FIFTEEN_MINS = \"15m\"\n",
    "    THIRTY_MINS = \"30m\"\n",
    "    HOUR = \"hour\"\n",
    "    DAY = \"day\"\n",
    "    WEEK = \"week\"\n",
    "    MONTH = \"month\"\n",
    "\n",
    "\n",
    "class CallerError(Exception):\n",
    "    \"\"\"\n",
    "    Error by caller of the client.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class PublicApiClient(RestAPIClient):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        environment: str,\n",
    "        api_key: str,\n",
    "        requests_per_sec_max: int,\n",
    "        logger: logging.Logger,\n",
    "    ):\n",
    "        match environment:\n",
    "            case \"production\" | \"prod\":\n",
    "                base_url = \"https://api-v3.wattwatchers.com.au\"\n",
    "            case \"staging\":\n",
    "                base_url = \"https://api-v3-stage.wattwatchers.com.au\"\n",
    "            case _:\n",
    "                # fallback is prod\n",
    "                base_url = \"https://api-v3.wattwatchers.com.au\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        # TODO: use a considered value for number of requests per second\n",
    "        super().__init__(base_url, requests_per_sec_max, headers=headers)\n",
    "        self._logger = logger\n",
    "\n",
    "    def get_devices_list(self) -> tuple[list | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Retrieves all device ids associated with the API key\n",
    "        \"\"\"\n",
    "        result = super().get(\"devices\")\n",
    "        return result\n",
    "\n",
    "    def get_device_status(self, device_id: str) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Retrieves the status of the device associated with the device_id\n",
    "        \"\"\"\n",
    "        result = super().get(f\"devices/{device_id}\")\n",
    "        return result\n",
    "\n",
    "    def patch_device_status(\n",
    "        self, device_id: str, payload: dict\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Patches the device status of the device associated with the device_id\n",
    "        Used (among other things) to update WiFi credentials\n",
    "        \"\"\"\n",
    "        result = super().patch(f\"devices/{device_id}\", data=json.dumps(payload))\n",
    "        return result\n",
    "\n",
    "    def update_wifi_credentials(\n",
    "        self, device_id: str, ssid: str | None = None, psk: str | None = None\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Updates the WiFi credentials of the device associated with the device_id\n",
    "        If successful, this will cause the device to switch to WiFi comms.\n",
    "        \"\"\"\n",
    "        if ssid is None and psk is None:\n",
    "            # No credential details provided, return error\n",
    "            return (\n",
    "                None,\n",
    "                CallerError(\n",
    "                    \"Request to update WiFi credentials requires at least one of SSID and PSK to be defined.\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        payload = {\"comms\": {\"wifi\": {}}}\n",
    "        if not (ssid is None):\n",
    "            payload[\"comms\"][\"wifi\"][\"ssid\"] = ssid\n",
    "        if not (psk is None):\n",
    "            payload[\"comms\"][\"wifi\"][\"psk\"] = psk\n",
    "\n",
    "        return self.patch_device_status(device_id, payload)\n",
    "\n",
    "    def reset_wifi_credentials(\n",
    "        self, device_id: str\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Resets the WiFi credentials of the device associated with the device_id\n",
    "        This will cause the device to switch to cellular comms.\n",
    "        \"\"\"\n",
    "        return self.update_wifi_credentials(device_id, \"\", \"\")\n",
    "\n",
    "    def change_switch_state(\n",
    "        self, device_id: str, switch_id: str, target_state: str\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Changes the switch state of the switch with id `switch_id` on the device with\n",
    "        id `device_id` to state `target_state`.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"id\": device_id,\n",
    "            \"switches\": [{\"id\": switch_id, \"state\": target_state}],\n",
    "        }\n",
    "        return self.patch_device_status(device_id, payload)\n",
    "\n",
    "    def update_se_reporting_interval(\n",
    "        self, device_id: str, reporting_interval: int\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        \"\"\"\n",
    "        Update the SE reporting interval for the device to the requested value\n",
    "        \"\"\"\n",
    "        payload = {\"shortEnergyReportingInterval\": reporting_interval}\n",
    "        return super().post(\n",
    "            f\"devices/{device_id}/reporting-interval\", data=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "    def get_latest_se(\n",
    "        self, device_id: str, energy_unit: str | None = \"kW\"\n",
    "    ) -> tuple[dict | None, RestError | None]:\n",
    "        if energy_unit is not None and energy_unit in [\"kW\", \"kWh\"]:\n",
    "            params = {\"convert[energy]\": energy_unit}\n",
    "            return super().get(f\"short-energy/{device_id}/latest\", params=params)\n",
    "        return super().get(f\"short-energy/{device_id}/latest\")\n",
    "\n",
    "    def _max_interval_for_granularity(self, granularity: Granularity) -> int:\n",
    "        \"\"\"\n",
    "        Returns the maximum interval for a single energy request based on the granularity\n",
    "        \"\"\"\n",
    "        MAX_INTERVALS_DAYS = {\n",
    "            Granularity.FIVE_MINS: 7,\n",
    "            Granularity.FIFTEEN_MINS: 14,\n",
    "            Granularity.THIRTY_MINS: 31,\n",
    "            Granularity.HOUR: 90,\n",
    "            Granularity.DAY: 3 * 365,  # ≈ 3 years\n",
    "            Granularity.WEEK: 5 * 365,  # ≈ 5 years\n",
    "            Granularity.MONTH: 10 * 365,  # ≈ 10 yers\n",
    "        }\n",
    "        return MAX_INTERVALS_DAYS.get(granularity, 7) * 24 * 3600\n",
    "\n",
    "    def _calculate_intervals_for(\n",
    "        self, granularity: Granularity, timestamp_start: int, timestamp_end: int\n",
    "    ) -> list[tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Batches an interval based on the maximum interval per request for the given granularity.\n",
    "        \"\"\"\n",
    "        batch_interval = self._max_interval_for_granularity(granularity)\n",
    "        intervals = [\n",
    "            TimeInterval(batch_start, min(batch_start + batch_interval, timestamp_end))\n",
    "            for batch_start in range(timestamp_start, timestamp_end, batch_interval)\n",
    "        ]\n",
    "        return intervals\n",
    "\n",
    "    def _load_energy(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        device_id: str,\n",
    "        intervals: list[TimeInterval],\n",
    "        unit: str = \"kWh\",\n",
    "        granularity: Granularity | None = None,\n",
    "    ) -> tuple[list | None, RestError | None]:\n",
    "\n",
    "        energy_data = []\n",
    "        for interval in intervals:\n",
    "            params = {\n",
    "                \"fromTs\": interval.timestamp_start,\n",
    "                \"toTs\": interval.timestamp_end,\n",
    "                \"convert[energy]\": unit,\n",
    "            }\n",
    "            if granularity is not None:\n",
    "                params[\"granularity\"] = granularity.value\n",
    "\n",
    "            self._logger.info(\n",
    "                f\"load from {interval.timestamp_start} to {interval.timestamp_end} for {device_id}\"\n",
    "            )\n",
    "            (result, error) = super().get(endpoint, params=params)\n",
    "            if error is not None:\n",
    "                self._logger.error(\n",
    "                    f\"Error retrieving LE data for {device_id} between {interval.timestamp_start} and {interval.timestamp_end}: {error}\"\n",
    "                )\n",
    "                return (None, error)\n",
    "\n",
    "            if result is not None:\n",
    "                energy_data.extend(result)\n",
    "\n",
    "        return (energy_data, None)\n",
    "\n",
    "    def load_long_energy(\n",
    "        self,\n",
    "        device_id: str,\n",
    "        timestamp_start: int,\n",
    "        timestamp_end: int,\n",
    "        granularity: Granularity = Granularity.FIVE_MINS,\n",
    "        unit: str = \"kWh\",\n",
    "    ) -> tuple[list | None, RestError | None]:\n",
    "\n",
    "        intervals = self._calculate_intervals_for(\n",
    "            granularity, timestamp_start, timestamp_end\n",
    "        )\n",
    "        return self._load_energy(\n",
    "            f\"long-energy/{device_id}\", device_id, intervals, unit, granularity\n",
    "        )\n",
    "\n",
    "    def get_first_le(self, device_id: str) -> tuple[list | None, RestError | None]:\n",
    "        result = super().get(f\"long-energy/{device_id}/first\")\n",
    "        return result\n",
    "\n",
    "    def load_short_energy(\n",
    "        self,\n",
    "        device_id: str,\n",
    "        timestamp_start: int,\n",
    "        timestamp_end: int,\n",
    "        unit: str = \"kWh\",\n",
    "    ) -> tuple[list | None, RestError | None]:\n",
    "\n",
    "        max_interval = 12 * 3600  # maximum request interval for SE is 12 hours\n",
    "        intervals = [\n",
    "            TimeInterval(batch_start, min(batch_start + max_interval, timestamp_end))\n",
    "            for batch_start in range(timestamp_start, timestamp_end, max_interval)\n",
    "        ]\n",
    "        return self._load_energy(\n",
    "            f\"short-energy/{device_id}\", device_id, intervals, unit\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger: logging.Logger = get_logger()\n",
    "public_api_client = PublicApiClient(ENVIRONMENT, API_KEY, MAX_TPS, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_device_id(device_id: str, raise_error: bool = False) -> tuple[bool, str]:\n",
    "  \"\"\"\n",
    "  Returns True if the device ID is valid, False if not.\n",
    "  Simplified version based on lib_common\n",
    "  \"\"\"\n",
    "  DEVICE_ID_PATTERN = \"^[B-F]{1}[A-F0-9]{12}$\"\n",
    "  DEVICE_ID_REGEX = re.compile(DEVICE_ID_PATTERN)\n",
    "  # re.match won't detect trailing space in the device id, but re.fullmatch will.\n",
    "  if not DEVICE_ID_REGEX.fullmatch(device_id):\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "# Determine devices to analyse\n",
    "devices: list[str] = []\n",
    "if DEVICE_IDS is not None and len(DEVICE_IDS) > 0:\n",
    "  devices = DEVICE_IDS\n",
    "else:\n",
    "  # get all devices associated with API key\n",
    "  result, error = public_api_client.get_devices_list()\n",
    "  if error is not None:\n",
    "    logger.error(f'failed to load devices for API key: {error}')\n",
    "  else:\n",
    "    devices = result\n",
    "\n",
    "# filter out any invalid device ids\n",
    "devices =[d for d in devices if is_valid_device_id(d)]\n",
    "\n",
    "\n",
    "num_devices = len(devices)\n",
    "logger.info(f'found {num_devices} devices to analyse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "End date 2025-03-25 needs to be in the past",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart date \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTART_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be earlier than end date \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEND_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_end \u001b[38;5;241m>\u001b[39m pendulum\u001b[38;5;241m.\u001b[39mnow():\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd date \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEND_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m needs to be in the past\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: End date 2025-03-25 needs to be in the past"
     ]
    }
   ],
   "source": [
    "# Determine timestamps for requests\n",
    "time_start = pendulum.parse(START_DATE, tz=TIMEZONE)\n",
    "timestamp_start = time_start.int_timestamp\n",
    "\n",
    "time_end = pendulum.parse(END_DATE, tz=TIMEZONE)\n",
    "timestamp_end = time_end.int_timestamp\n",
    "\n",
    "if time_start > time_end:\n",
    "    raise ValueError(f\"Start date ({START_DATE}) has to be earlier than end date ({END_DATE})\")\n",
    "if time_end > pendulum.now():\n",
    "    raise ValueError(f\"End date ({END_DATE}) needs to be in the past\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load device status for each device to exclude decommissioned devices\n",
    "# Issue: only user-apps-api exposes this and we can't use that because of different auth system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LE data\n",
    "le_data = {}\n",
    "for index, device_id in enumerate(devices):\n",
    "  logger.info(f'Downloading LE data for device {index+1}/{num_devices} - {device_id}')\n",
    "  result, error = public_api_client.load_long_energy(device_id, timestamp_start, timestamp_end)\n",
    "  if error is not None:\n",
    "    logger.error(f'failed to load LE for device: {device_id}: {error}')\n",
    "  else:\n",
    "    le_data[device_id] = result\n",
    "\n",
    "logger.info(f'Successfully downloaded LE data for {len(le_data)}/{num_devices} devices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether le_data dict is empty\n",
    "if not le_data:\n",
    "    raise ValueError(\"No LE data was downloaded for any device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices we couldn't download LE data for (included in devices list but not in LE dict)\n",
    "devices_with_le_data = list(le_data.keys())\n",
    "devices_without_le_data = [device_id for device_id in devices if device_id not in devices_with_le_data]\n",
    "\n",
    "devices_with_empty_le_data = [device_id for device_id, data in le_data.items() if len(data) == 0]\n",
    "\n",
    "# Determine expected number of intervals\n",
    "# Calculate the number of 5-minute intervals between the start and end timestamps using pendulum\n",
    "num_intervals_expected = int((time_end.diff(time_start).in_minutes()) // 5)\n",
    "\n",
    "# Devices with missing LE data\n",
    "# TODO: add alternative analysis based on timestamp and duration of interval (only works for intervals between existing intervals, i.e. need to handle missing intervals at start or end of period separately)\n",
    "# Could also just do a quick analysis to verify all intervals have a duration of 300s.\n",
    "devices_with_missing_le_data = {device_id: data for device_id, data in le_data.items() if len(data) < num_intervals_expected}\n",
    "\n",
    "# Devices not meeting data completeness threshold\n",
    "num_intervals_completeness_threshold = DATA_COMPLETENESS_THRESHOLD * num_intervals_expected // 100 # TODO: double check if this can result in off-by-one error\n",
    "devices_not_meeting_threshold = {device_id: data for device_id, data in le_data.items() if len(data) < num_intervals_completeness_threshold}\n",
    "\n",
    "# Devices with complete LE data\n",
    "devices_with_complete_le_data = {device_id: data for device_id, data in le_data.items() if len(data) == num_intervals_expected}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High level analysis\n",
    "\n",
    "le_data_df = pd.DataFrame({\n",
    "    'device_id': list(le_data.keys()),\n",
    "    'le_intervals': list(le_data.values()),\n",
    "    'num_intervals': [len(data) for data in le_data.values()]\n",
    "})\n",
    "\n",
    "le_data_df['num_intervals_expected'] = num_intervals_expected\n",
    "le_data_df['num_intervals_missing'] = le_data_df['num_intervals_expected'] - le_data_df['num_intervals']\n",
    "le_data_df['interval_completeness'] = le_data_df['num_intervals'] / le_data_df['num_intervals_expected']\n",
    "le_data_df['interval_missingness'] = 1 - le_data_df['interval_completeness']\n",
    "\n",
    "le_data_df = le_data_df.sort_values(by='interval_completeness')\n",
    "\n",
    "df_devices_table = le_data_df[['device_id', 'interval_completeness', 'num_intervals_missing']].copy()\n",
    "df_devices_table['interval_completeness'] = df_devices_table['interval_completeness'] * 100\n",
    "\n",
    "parameters = [{\n",
    "  'start_time': time_start,\n",
    "  'end_time': time_end,\n",
    "  'num_expected_intervals': num_intervals_expected,\n",
    "  \n",
    "}]\n",
    "\n",
    "top_level_stats = [{\n",
    "  'num_devices': num_devices,\n",
    "  'overall_completeness': le_data_df['num_intervals'].sum() / (num_devices * num_intervals_expected) * 100,\n",
    "  'devices_under_threshold': len(devices_not_meeting_threshold),\n",
    "  'devices_with_missing_intervals': len(devices_with_missing_le_data),\n",
    "  'devices_without_data': len(devices_with_empty_le_data),\n",
    "  'devices_with_failed_retrieval': len(devices_without_le_data),\n",
    "  'devices_with_complete_data': len(devices_with_complete_le_data),\n",
    "}]\n",
    "\n",
    "df_parameters = pd.DataFrame.from_dict(parameters)\n",
    "df_stats = pd.DataFrame.from_dict(top_level_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform interval data\n",
    "\n",
    "def flatten_arrays(item: dict) -> dict:\n",
    "    \"\"\" flatten each element of arrays to their own key. Other types of values are left untouched.\n",
    "    e.g. {key: [value0, value1, ...]} becomes {key_0: value0, key_1: value1, ...}.            \n",
    "    \"\"\"\n",
    "    flattened = {}\n",
    "    for key, value in item.items():\n",
    "      if isinstance(value, list):\n",
    "        for idx, subvalue in enumerate(value):\n",
    "          flattened[f\"{key}_{idx}\"] = subvalue\n",
    "      else:\n",
    "        flattened[key] = value\n",
    "    return flattened\n",
    "\n",
    "\n",
    "# Create an empty DataFrame\n",
    "intervals = []\n",
    "\n",
    "for index, row in le_data_df.iterrows():\n",
    "    # Perform any necessary operations on each row\n",
    "    # For example, you could print the device_id and interval_completeness\n",
    "    device_id = row['device_id']\n",
    "    data = row['le_intervals']\n",
    "    for item in data:\n",
    "        row = flatten_arrays(item)\n",
    "        row[\"device_id\"] = device_id\n",
    "        intervals.append(row)\n",
    "\n",
    "df_intervals = pd.DataFrame.from_dict(intervals) \n",
    "# Reorder the columns to move 'device_id', 'timestamp', and 'duration' to the front\n",
    "columns_order = ['device_id', 'timestamp', 'duration'] + [col for col in df_intervals.columns if col not in ['device_id', 'timestamp', 'duration']]\n",
    "df_intervals = df_intervals[columns_order]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True, at position 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m df_daily_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval_completeness\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m df_daily_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_intervals\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m num_intervals_expected_daily\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Move interval_completeness column to 3rd column\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# df_daily_counts['date'] = pd.to_datetime(df_daily_counts['date']).dt.date\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df_daily_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_daily_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\n\u001b[1;32m     28\u001b[0m cols \u001b[38;5;241m=\u001b[39m df_daily_counts\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     29\u001b[0m cols\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m2\u001b[39m, cols\u001b[38;5;241m.\u001b[39mpop(cols\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval_completeness\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/le-completeness-analysis-XK_cgeV8-py3.12/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/le-completeness-analysis-XK_cgeV8-py3.12/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[0;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/le-completeness-analysis-XK_cgeV8-py3.12/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:435\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m--> 435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     out_unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/le-completeness-analysis-XK_cgeV8-py3.12/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py:2398\u001b[0m, in \u001b[0;36mobjects_to_datetime64\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n\u001b[0;32m-> 2398\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2408\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[1;32m   2410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "File \u001b[0;32mtslib.pyx:414\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtslib.pyx:596\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtslib.pyx:504\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:285\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.DatetimeParseState.process_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True, at position 5"
     ]
    }
   ],
   "source": [
    "# By-day analysis\n",
    "\n",
    "df_intervals['datetime'] = pd.to_datetime(df_intervals['timestamp'], unit='s').dt.tz_localize('UTC').dt.tz_convert(TIMEZONE)\n",
    "\n",
    "df_daily_counts = df_intervals.groupby(['device_id', df_intervals['datetime'].dt.date]).size().reset_index(name='entry_count')\n",
    "df_daily_counts.columns = ['device_id', 'date', 'num_intervals']\n",
    "df_daily_counts.head(70)\n",
    "df_daily_counts['date'] = pd.to_datetime(df_daily_counts['date']).dt.tz_localize(TIMEZONE)\n",
    "\n",
    "# Add in missing intervals (set to 0)\n",
    "date_range = pd.date_range(start=time_start, end=time_end.subtract(seconds=1), freq='D')\n",
    "all_device_dates = pd.MultiIndex.from_product([devices, date_range], names=['device_id', 'date'])\n",
    "\n",
    "missing_entries = all_device_dates.difference(df_daily_counts.set_index(['device_id', 'date']).index)\n",
    "missing_df = pd.DataFrame(list(missing_entries), columns=['device_id', 'date'])\n",
    "missing_df['num_intervals'] = 0\n",
    "df_daily_counts = pd.concat([df_daily_counts, missing_df], ignore_index=True)\n",
    "\n",
    "num_intervals_expected_daily = 24 * 12\n",
    "df_daily_counts['missing_intervals'] = num_intervals_expected_daily - df_daily_counts['num_intervals']\n",
    "df_daily_counts['interval_completeness'] = 100 * df_daily_counts['num_intervals'] / num_intervals_expected_daily\n",
    "# Move interval_completeness column to 3rd column\n",
    "\n",
    "# df_daily_counts['date'] = pd.to_datetime(df_daily_counts['date']).dt.date\n",
    "df_daily_counts['date'] = pd.to_datetime(df_daily_counts['date']).dt.date\n",
    "\n",
    "\n",
    "cols = df_daily_counts.columns.tolist()\n",
    "cols.insert(2, cols.pop(cols.index('interval_completeness')))\n",
    "df_daily_counts = df_daily_counts[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df_parameters)\n",
    "show(df_stats, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [1], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [1],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per device stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df_devices_table, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [1], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [1],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],\n",
    "    showIndex=False,\n",
    "    buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per device per day stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df_daily_counts, \n",
    "     columnDefs= [\n",
    "        { \"targets\": [2], \"createdCell\": JavascriptFunction(\n",
    "                f\"\"\"\n",
    "                    function (td, cellData, rowData, row, col) {{\n",
    "                        if (cellData < {DATA_COMPLETENESS_THRESHOLD}) {{\n",
    "                            $(td).css('color', 'red')\n",
    "                        }}\n",
    "                    }}\n",
    "                \"\"\"\n",
    "        )},\n",
    "        {\n",
    "            \"targets\": [2],\n",
    "            \"render\": JavascriptCode(\"$.fn.dataTable.render.number(',', '.', 2, '', '%')\"),\n",
    "        }\n",
    "    ],\n",
    "    showIndex=False,\n",
    "    pageLength=20,\n",
    "    buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For large datasets, running this cell may result in memory issues.\n",
    "\n",
    "show(df_intervals, \n",
    "     showIndex=False,\n",
    "     maxBytes=0,\n",
    "     pageLength=20,\n",
    "     buttons=[\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    "     )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "le-completeness-analysis-XK_cgeV8-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
